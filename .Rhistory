kable(
data.frame("Original AUC" = round(original_AUC, 3),
"Adjusted AUC" = round(adjusted_AUC, 3),
"95% CI for Adjusted AUC" = c(round(adjusted_auc_ci[1], 3), round(adjusted_auc_ci[2], 3)),
col.names = c("Original AUC", "Adjusted AUC", "95% CI for Adjusted AUC"),
caption = "Comparison of Original and Adjusted AUC with its 95% CI"))
# Print the original AUC and the adjusted AUC and the 95% CI
kable(
data.frame("Original AUC" = round(original_AUC, 3),
"Adjusted AUC" = round(adjusted_AUC, 3),
"95% CI for Adjusted AUC" = c(round(adjusted_auc_ci[1], 3), round(adjusted_auc_ci[2], 3)),
row.names = c("Original AUC", "Adjusted AUC", "95% CI for Adjusted AUC"),
caption = "Comparison of Original and Adjusted AUC with its 95% CI"))
# Print the original AUC and the adjusted AUC and the 95% CI
kable(
data.frame("Original AUC" = original_AUC, "Adjusted AUC" = adjusted_AUC,
"95% CI for Adjusted AUC" = c(adjusted_auc_ci[1], adjusted_auc_ci[2]),
row.names = c("Original AUC", "Adjusted AUC", "95% CI for Adjusted AUC"),
caption = "Comparison of Original and Adjusted AUC with its 95% CI"))
original_AUC
validation_result[, "index.orig"]
validation_result[1, "index.orig"]
# AUC adjustment using validate function (bootstrap method)
validation_result <- validate(final_model, method = "boot", B = 200)
# View the structure of the validation_result
summary(validation_result)
# Extract the original AUC (index.orig) and the adjusted AUC (index.corrected)
index_orig <- validation_result[1, "index.orig"]   # Somers' Dxy
index_corrected <- validation_result[1, "index.corrected"]  # Optimism-corrected Somers' Dxy
original_AUC <- 0.5*(index_orig + 1)  # Calculate the original AUC
adjusted_AUC <- 0.5*(index_corrected + 1)  # Calculate the adjusted AUC
# Calculate the 95% confidence interval for the adjusted AUC
adjusted_auc_ci <- quantile(index_corrected, c(0.025, 0.975))
# AUC adjustment using validate function (bootstrap method)
validation_result <- validate(final_model, method = "boot", B = 200)
# View the structure of the validation_result
validation_result
# Extract the original AUC (index.orig) and the adjusted AUC (index.corrected)
index_orig <- validation_result[1, "index.orig"]   # Somers' Dxy
index_corrected <- validation_result[1, "index.corrected"]  # Optimism-corrected Somers' Dxy
original_AUC <- 0.5*(index_orig + 1)  # Calculate the original AUC
adjusted_AUC <- 0.5*(index_corrected + 1)  # Calculate the adjusted AUC
# Calculate the 95% confidence interval for the adjusted AUC
adjusted_auc_ci <- quantile(index_corrected, c(0.025, 0.975))
# Print the original AUC and the adjusted AUC and the 95% CI
kable(
data.frame("Original AUC" = original_AUC, "Adjusted AUC" = adjusted_AUC,
"95% CI for Adjusted AUC" = c(adjusted_auc_ci[1], adjusted_auc_ci[2]),
row.names = c("Original AUC", "Adjusted AUC", "95% CI for Adjusted AUC"),
caption = "Comparison of Original and Adjusted AUC with its 95% CI"))
# Print the original AUC and the adjusted AUC and the 95% CI
kable(
data.frame("Original AUC" = original_AUC, "Adjusted AUC" = adjusted_AUC,
"95% CI for Adjusted AUC" = c(adjusted_auc_ci[1], adjusted_auc_ci[2]),
row.names = c("Original AUC", "Adjusted AUC", "95% CI for Adjusted AUC"),
caption = "Comparison of Original and Adjusted AUC with its 95% CI"))
# Print the original AUC and the adjusted AUC and the 95% CI
kable(
data.frame("Original AUC" = original_AUC, "Adjusted AUC" = adjusted_AUC,
"95% CI for Adjusted AUC" = c(adjusted_auc_ci[1], adjusted_auc_ci[2]),
row.names = c("Original AUC", "Adjusted AUC", "95% CI for Adjusted AUC"),
caption = "Comparison of Original and Adjusted AUC with its 95% CI"))
# Print the original AUC and the adjusted AUC and the 95% CI
kable(
data.frame("Original AUC" = original_AUC, "Adjusted AUC" = adjusted_AUC,
"95% CI for Adjusted AUC" = c(adjusted_auc_ci[1], adjusted_auc_ci[2]),
caption = "Comparison of Original and Adjusted AUC with its 95% CI"))
# Print the original AUC and the adjusted AUC and the 95% CI
kable(
data.frame("Original AUC" = original_AUC, "Adjusted AUC" = adjusted_AUC,
"Lower 95% CI for Adjusted AUC" = adjusted_auc_ci[1],
"Upper 95% CI for Adjusted AUC" = adjusted_auc_ci[2]),
caption = "Comparison of Original and Adjusted AUC with its 95% CI"))
# Print the original AUC and the adjusted AUC and the 95% CI
kable(
data.frame("Original AUC" = original_AUC, "Adjusted AUC" = adjusted_AUC,
"Lower 95% CI for Adjusted AUC" = adjusted_auc_ci[1],
"Upper 95% CI for Adjusted AUC" = adjusted_auc_ci[2]),
caption = "Comparison of Original and Adjusted AUC with its 95% CI")
# AUC adjustment using validate function (bootstrap method)
validation_result <- validate(final_model, method = "boot", B = 200)
# View the structure of the validation_result
validation_result
# Extract the original AUC (index.orig) and the adjusted AUC (index.corrected)
index_orig <- validation_result[1, "index.orig"]   # Somers' Dxy
index_corrected <- validation_result[1, "index.corrected"]  # Optimism-corrected Somers' Dxy
original_AUC <- 0.5*(index_orig + 1)  # Calculate the original AUC
adjusted_AUC <- 0.5*(index_corrected + 1)  # Calculate the adjusted AUC
# Calculate the 95% confidence interval for the adjusted AUC
adjusted_auc_ci <- quantile(adjusted_AUC, c(0.025, 0.975))
# AUC adjustment using validate function (bootstrap method)
validation_result <- validate(final_model, method = "boot", B = 200)
# View the structure of the validation_result
validation_result
# Extract the original AUC (index.orig) and the adjusted AUC (index.corrected)
index_orig <- validation_result[1, "index.orig"]   # Somers' Dxy
index_corrected <- validation_result[1, "index.corrected"]  # Optimism-corrected Somers' Dxy
original_AUC <- 0.5*(index_orig + 1)  # Calculate the original AUC
adjusted_AUC <- 0.5*(index_corrected + 1)  # Calculate the adjusted AUC
# Calculate the 95% confidence interval for the adjusted AUC
adjusted_auc_ci <- confint(adjusted_AUC, c(0.025, 0.975))
# AUC adjustment using validate function (bootstrap method)
validation_result <- validate(final_model, method = "boot", B = 200)
# View the structure of the validation_result
validation_result
# Extract the original AUC (index.orig) and the adjusted AUC (index.corrected)
index_orig <- validation_result[1, "index.orig"]   # Somers' Dxy
index_corrected <- validation_result[1, "index.corrected"]  # Optimism-corrected Somers' Dxy
original_AUC <- 0.5*(index_orig + 1)  # Calculate the original AUC
adjusted_AUC <- 0.5*(index_corrected + 1)  # Calculate the adjusted AUC
# Calculate the 95% confidence interval for the adjusted AUC
adjusted_auc_ci <- confint(adjusted_AUC, level = 0.95)
adjusted_AUC
adjusted_auc_ci <- confint(index_corrected, level = 0.95)
confint(validation_result$)
confint(validation_result[, "index.corrected"] , level = 0.95)
confint(validation_result, "index.corrected" , level = 0.95)
quantile(validation_result[, "index.corrected"] , c(0.025, 0.975))
# Print the original AUC and the adjusted AUC and the 95% CI
kable(
data.frame("Original" = original_AUC, "Adjusted" = adjusted_AUC,
"2.5Q_Adjusted" = adjusted_auc_ci[1],
"97.5Q_Adjusted" = adjusted_auc_ci[2]),
row.names = "AUC"
caption = "Comparison of Original and Adjusted AUC with its 95% CI")
# Print the original AUC and the adjusted AUC and the 95% CI
kable(
data.frame("Original" = original_AUC, "Adjusted" = adjusted_AUC,
"2.5Q_Adjusted" = adjusted_auc_ci[1],
"97.5Q_Adjusted" = adjusted_auc_ci[2]),
row.names = c("AUC"),
caption = "Comparison of Original and Adjusted AUC with its 95% CI")
# Print the original AUC and the adjusted AUC and the 95% CI
kable(
data.frame("Original" = original_AUC, "Adjusted" = adjusted_AUC,
"2.5Q_Adjusted" = adjusted_auc_ci[1],
"97.5Q_Adjusted" = adjusted_auc_ci[2]),
row.names = c("AUC"),
col.names = c("Original AUC", "Adjusted AUC", "2.5% CI", "97.5% CI"),
caption = "Comparison of Original and Adjusted AUC with its 95% CI")
# Print the original AUC and the adjusted AUC and the 95% CI
kable(
data.frame(original_AUC, adjusted_AUC, adjusted_auc_ci[1],adjusted_auc_ci[2]),
row.names = c("AUC"),
col.names = c("Original AUC", "Adjusted AUC", "2.5% CI", "97.5% CI"),
caption = "Comparison of Original and Adjusted AUC with its 95% CI")
# Print the original AUC and the adjusted AUC and the 95% CI
kable(
data.frame(original_AUC, adjusted_AUC, adjusted_auc_ci[1],adjusted_auc_ci[2]),
col.names = c("Original AUC", "Adjusted AUC", "2.5% CI", "97.5% CI"),
caption = "Comparison of Original and Adjusted AUC with its 95% CI")
# Print the original AUC and the adjusted AUC and the 95% CI
kable(
data.frame(original_AUC, adjusted_AUC, adjusted_auc_ci[1],adjusted_auc_ci[2]),
col.names = c("Original AUC", "Adjusted AUC", "2.5% CI", "97.5% CI"),
row.names = FALSE,
caption = "Comparison of Original and Adjusted AUC with its 95% CI")
# AUC adjustment using validate function (bootstrap method)
validation_result <- validate(final_model, method = "boot", B = 200)
# View the structure of the validation_result
validation_result
# Extract the original AUC (index.orig) and the adjusted AUC (index.corrected)
index_orig <- validation_result[1, "index.orig"]   # Somers' Dxy
index_corrected <- validation_result[1, "index.corrected"]  # Optimism-corrected Somers' Dxy
original_AUC <- 0.5*(index_orig + 1)  # Calculate the original AUC
adjusted_AUC <- 0.5*(index_corrected + 1)  # Calculate the adjusted AUC
# Calculate the 95% confidence interval for the adjusted AUC
adjusted_auc_ci <- quantile(validation_result[, "index.corrected"] , c(0.025, 0.975))
# AUC adjustment using validate function (bootstrap method)
validation_result <- validate(final_model, method = "boot", B = 200)
# View the structure of the validation_result
validation_result
# Extract the original AUC (index.orig) and the adjusted AUC (index.corrected)
index_orig <- validation_result[1, "index.orig"]   # Somers' Dxy
index_corrected <- validation_result[1, "index.corrected"]
# Optimism-corrected Somers' Dxy
original_AUC <- 0.5*(index_orig + 1)  # Calculate the original AUC
adjusted_AUC <- 0.5*(index_corrected + 1)  # Calculate the adjusted AUC
# Calculate the 95% confidence interval for the adjusted AUC using bootstrap samples
auc_samples <- replicate(200, {
# Resample data
boot_data <- di[sample(nrow(di), replace = TRUE), ]
# Fit the model on the bootstrapped data
boot_model <- lrm(chd69 ~ dibpat0f + age0 + cholmmol + sbp10 + bmi + smokerf
+ arcus0 + bmi * sbp10, age0*arcus0, data = boot_data,
x = TRUE, y = TRUE)
# Calculate the AUC for the bootstrapped model
roc_curve_boot <- roc(boot_data$chd69, predict(boot_model, boot_data,
type = "fitted"))
auc(roc_curve_boot)
})
# AUC adjustment using validate function (bootstrap method)
validation_result <- validate(final_model, method = "boot", B = 200)
# View the structure of the validation_result
validation_result
# Extract the original AUC (index.orig) and the adjusted AUC (index.corrected)
index_orig <- validation_result[1, "index.orig"]   # Somers' Dxy
index_corrected <- validation_result[1, "index.corrected"]
# Optimism-corrected Somers' Dxy
original_AUC <- 0.5*(index_orig + 1)  # Calculate the original AUC
adjusted_AUC <- 0.5*(index_corrected + 1)  # Calculate the adjusted AUC
# Print the original AUC and the adjusted AUC and the 95% CI
kable(
data.frame(original_AUC, adjusted_AUC),
col.names = c("Original AUC", "Adjusted AUC"),
row.names = FALSE,
caption = "Comparison of Original and Adjusted AUC")
# Set the number of folds for cross-validation
num_folds <- 10
# Create a 10-fold cross-validation partition
folds <- createFolds(di$chd69, k = num_folds, list = TRUE)
# Store the AUC values
auc_values <- c()
# Perform 10-fold cross-validation
for (i in 1:num_folds) {
# Define training and testing sets
train_data <- di[folds[[i]], ]
test_data <- di[-folds[[i]], ]
# Fit the logistic model on the training data
model_cv <- lrm(chd69 ~ dibpat0f + age0 + cholmmol + sbp10 + bmi + smokerf + arcus0, data = train_data, x = TRUE, y = TRUE)
# Predict on the test set
predicted_prob <- predict(model_cv, test_data, type = "fitted")
# Calculate the AUC for the current fold
roc_curve_cv <- roc(test_data$chd69, predicted_prob)
auc_values[i] <- auc(roc_curve_cv)
}
# Calculate the average AUC from the cross-validation
avg_auc <- mean(auc_values)
ci_auc <- quantile(auc_values, probs = c(0.025, 0.975))
# Print the results
print(paste("Average AUC from 10-fold CV: ", round(avg_auc, 3)))
print(paste("95% CI for cross-validated AUC: ", round(ci_auc[1], 3), " to ", round(ci_auc[2], 3)))
# Compare the cross-validated AUC with the Unadjusted
print(paste("Unadjusted AUC: ", round(auc_value, 3)))
print(paste("95% CI for Unadjusted AUC: ", round(auc_ci[1], 3), " to ", round(auc_ci[2], 3)))
# Set the number of folds for cross-validation
num_folds <- 10
# Create a 10-fold cross-validation partition
folds <- createFolds(di$chd69, k = num_folds, list = TRUE)
# Store the AUC values
auc_values <- c()
# Perform 10-fold cross-validation
for (i in 1:num_folds) {
# Define training and testing sets
train_data <- di[folds[[i]], ]
test_data <- di[-folds[[i]], ]
# Fit the logistic model on the training data
model_cv <- lrm(chd69 ~ dibpat0f + age0 + cholmmol + sbp10 + bmi + smokerf
+ arcus0 + bmi * sbp10, age0*arcus0 , data = train_data,
x = TRUE, y = TRUE)
# Predict on the test set
predicted_prob <- predict(model_cv, test_data, type = "fitted")
# Calculate the AUC for the current fold
roc_curve_cv <- roc(test_data$chd69, predicted_prob)
auc_values[i] <- auc(roc_curve_cv)
}
# Set the number of folds for cross-validation
num_folds <- 10
# Create a 10-fold cross-validation partition
folds <- createFolds(di$chd69, k = num_folds, list = TRUE)
# Store the AUC values
auc_values <- c()
# Perform 10-fold cross-validation
for (i in 1:num_folds) {
# Define training and testing sets
train_data <- di[folds[[i]], ]
test_data <- di[-folds[[i]], ]
# Fit the logistic model on the training data
model_cv <- lrm(chd69 ~ dibpat0f + age0 + cholmmol + sbp10 + bmi + smokerf
+ arcus0 + bmi * sbp10 + age0*arcus0 , data = train_data,
x = TRUE, y = TRUE)
# Predict on the test set
predicted_prob <- predict(model_cv, test_data, type = "fitted")
# Calculate the AUC for the current fold
roc_curve_cv <- roc(test_data$chd69, predicted_prob)
auc_values[i] <- auc(roc_curve_cv)
}
# Calculate the average AUC from the cross-validation
avg_auc <- mean(auc_values)
ci_auc <- quantile(auc_values, probs = c(0.025, 0.975))
# Print the results
print(paste("Average AUC from 10-fold CV: ", round(avg_auc, 3)))
print(paste("95% CI for cross-validated AUC: ", round(ci_auc[1], 3), " to ", round(ci_auc[2], 3)))
# Compare the cross-validated AUC with the Unadjusted
kable(
data.frame(avg_auc, original_AUC, adjusted_AUC),
col.names = c("Cross-Validated AUC", "Unadjusted AUC", "Adjusted AUC"),
row.names = FALSE,
caption = "Comparison of Cross-Validated, Unadjusted and Adjusted AUC")
# Set the number of folds for cross-validation
num_folds <- 10
# Create a 10-fold cross-validation partition
folds <- createFolds(di$chd69, k = num_folds, list = TRUE)
# Store the AUC values
auc_values <- c()
# Perform 10-fold cross-validation
for (i in 1:num_folds) {
# Define training and testing sets
train_data <- di[folds[[i]], ]
test_data <- di[-folds[[i]], ]
# Fit the logistic model on the training data
model_cv <- lrm(chd69 ~ dibpat0f + age0 + cholmmol + sbp10 + bmi + smokerf
+ arcus0 + bmi * sbp10 + age0*arcus0 , data = train_data,
x = TRUE, y = TRUE)
# Predict on the test set
predicted_prob <- predict(model_cv, test_data, type = "fitted")
# Calculate the AUC for the current fold
roc_curve_cv <- roc(test_data$chd69, predicted_prob)
auc_values[i] <- auc(roc_curve_cv)
}
# Calculate the average AUC from the cross-validation
avg_auc <- mean(auc_values)
ci_auc <- quantile(auc_values, probs = c(0.025, 0.975))
# Print the results
print(paste("Average AUC from 10-fold CV: ", round(avg_auc, 3)))
print(paste("95% CI for cross-validated AUC: ", round(ci_auc[1], 3), " to ", round(ci_auc[2], 3)))
# Compare the cross-validated AUC with the Unadjusted
kable(
data.frame(avg_auc, original_AUC, adjusted_AUC),
col.names = c("Cross-Validated AUC", "Unadjusted AUC", "Adjusted AUC"),
row.names = FALSE,
caption = "Comparison of Cross-Validated, Unadjusted and Adjusted AUC")
# Print the results
print(paste("Average AUC from 10-fold CV: ", round(avg_auc, 3)))
print(paste("95% CI for cross-validated AUC: ", round(ci_auc[1], 3), " to ", round(ci_auc[2], 3)))
# Compare the cross-validated AUC with the Unadjusted
kable(
data.frame(avg_auc, original_AUC, adjusted_AUC, ci_auc[1], ci_auc[2]),
col.names = c("Cross-Validated AUC", "Unadjusted AUC", "Adjusted AUC", "CI Lower", "CI Upper"),
row.names = FALSE,
caption = "Comparison of Cross-Validated, Unadjusted and Adjusted AUC")
knitr::opts_chunk$set(echo = TRUE)
#install.packages("pacman")
pacman::p_load(tidyverse,rms,haven,mgcv,epitools,logistf,nlpred,geepack,skimr,pROC,tableone,emmeans,glmtoolbox,CalibrationCurves,mice, dcurves,caret)
library(tableone)
library(ResourceSelection)
set.seed(154550)
# Compare the cross-validated AUC with the Unadjusted
knitr::kable(
data.frame(avg_auc, original_AUC, adjusted_AUC, ci_auc[1], ci_auc[2]),
col.names = c("Cross-Validated AUC", "Unadjusted AUC", "Adjusted AUC", "CI Lower", "CI Upper"),
row.names = FALSE,
caption = "Comparison of Cross-Validated, Unadjusted and Adjusted AUC")
# Set the number of folds for cross-validation
num_folds <- 10
# Create a 10-fold cross-validation partition
folds <- createFolds(di$chd69, k = num_folds, list = TRUE)
# Store the AUC values
auc_values <- c()
# Perform 10-fold cross-validation
for (i in 1:num_folds) {
# Define training and testing sets
train_data <- di[folds[[i]], ]
test_data <- di[-folds[[i]], ]
# Fit the logistic model on the training data
model_cv <- lrm(chd69 ~ dibpat0f + age0 + cholmmol + sbp10 + bmi + smokerf
+ arcus0 + bmi * sbp10 + age0*arcus0 , data = train_data,
x = TRUE, y = TRUE)
# Predict on the test set
predicted_prob <- predict(model_cv, test_data, type = "fitted")
# Calculate the AUC for the current fold
roc_curve_cv <- roc(test_data$chd69, predicted_prob)
auc_values[i] <- auc(roc_curve_cv)
}
# Calculate the average AUC from the cross-validation
avg_auc <- mean(auc_values)
ci_auc <- quantile(auc_values, probs = c(0.025, 0.975))
# Compare the cross-validated AUC with the Unadjusted
knitr::kable(
data.frame(avg_auc, original_AUC, adjusted_AUC, ci_auc[1], ci_auc[2]),
col.names = c("Cross-Validated AUC", "Unadjusted AUC", "Adjusted AUC", "CI Lower", "CI Upper"),
row.names = FALSE,
caption = "Comparison of Cross-Validated, Unadjusted and Adjusted AUC")
set.seed(154550)
# Set the number of folds for cross-validation
num_folds <- 10
# Create a 10-fold cross-validation partition
folds <- createFolds(di$chd69, k = num_folds, list = TRUE)
# Store the AUC values
auc_values <- c()
# Perform 10-fold cross-validation
for (i in 1:num_folds) {
# Define training and testing sets
train_data <- di[folds[[i]], ]
test_data <- di[-folds[[i]], ]
# Fit the logistic model on the training data
model_cv <- lrm(chd69 ~ dibpat0f + age0 + cholmmol + sbp10 + bmi + smokerf
+ arcus0 + bmi * sbp10 + age0*arcus0 , data = train_data,
x = TRUE, y = TRUE)
# Predict on the test set
predicted_prob <- predict(model_cv, test_data, type = "fitted")
# Calculate the AUC for the current fold
roc_curve_cv <- roc(test_data$chd69, predicted_prob)
auc_values[i] <- auc(roc_curve_cv)
}
# Calculate the average AUC from the cross-validation
avg_auc <- mean(auc_values)
ci_auc <- quantile(auc_values, probs = c(0.025, 0.975))
# Compare the cross-validated AUC with the Unadjusted
knitr::kable(
data.frame(avg_auc, original_AUC, adjusted_AUC, ci_auc[1], ci_auc[2]),
col.names = c("Cross-Validated AUC", "Unadjusted AUC", "Adjusted AUC", "CI Lower", "CI Upper"),
row.names = FALSE,
caption = "Comparison of Cross-Validated, Unadjusted and Adjusted AUC")
set.seed(154550)
# Set the number of folds for cross-validation
num_folds <- 10
# Create a 10-fold cross-validation partition
folds <- createFolds(di$chd69, k = num_folds, list = TRUE)
# Store the AUC values
auc_values <- c()
# Perform 10-fold cross-validation
for (i in 1:num_folds) {
# Define training and testing sets
train_data <- di[folds[[i]], ]
test_data <- di[-folds[[i]], ]
# Fit the logistic model on the training data
model_cv <- lrm(chd69 ~ dibpat0f + age0 + cholmmol + sbp10 + bmi + smokerf
+ arcus0 + bmi * sbp10 + age0*arcus0 , data = train_data,
x = TRUE, y = TRUE)
# Predict on the test set
predicted_prob <- predict(model_cv, test_data, type = "fitted")
# Calculate the AUC for the current fold
roc_curve_cv <- roc(test_data$chd69, predicted_prob)
auc_values[i] <- auc(roc_curve_cv)
}
# Calculate the average AUC from the cross-validation
avg_auc <- mean(auc_values)
ci_auc <- quantile(auc_values, probs = c(0.025, 0.975))
# Compare the cross-validated AUC with the Unadjusted
knitr::kable(
data.frame(avg_auc, original_AUC, adjusted_AUC, ci_auc[1], ci_auc[2]),
col.names = c("Cross-Validated AUC", "Unadjusted AUC", "Adjusted AUC", "CI Lower", "CI Upper"),
row.names = FALSE,
caption = "Comparison of Cross-Validated, Unadjusted and Adjusted AUC")
# Plot the calibration curve
cal <- calibrate(final_model, method = "boot", B = 1000)
plot(cal, main = "Calibration Curve")
# Extract calibration points
cal_points <- as.data.frame(cal[, c("predy", "calibrated.corrected")])
names(cal_points) <- c("Predicted", "Observed")
# Fit the logistic regression model of the calibration points and calculate the slope and intercept
cal_model <- lrm(Observed ~ Predicted, data = cal_points)
calibration_slope <- coef(cal_model)["Predicted"]
calibration_intercept <- coef(cal_model)["(Intercept)"]
# Report the slope and the intercept of the calibration curve
cat("Calibration Slope:", calibration_slope, "\n")
cat("Calibration Intercept:", calibration_intercept, "\n")
# Plot the calibration curve
cal <- calibrate(final_model, method = "boot", B = 1000)
plot(cal, main = "Calibration Curve")
# Extract calibration points
cal_points <- as.data.frame(cal[, c("predy", "calibrated.corrected")])
names(cal_points) <- c("Predicted", "Observed")
# Fit the logistic regression model of the calibration points and calculate the slope and intercept
cal_model <- lrm(Observed ~ Predicted, data = cal_points)
calibration_slope <- coef(cal_model)["Predicted"]
calibration_intercept <- coef(cal_model)["(Intercept)"]
# Report the slope and the intercept of the calibration curve
cat("Calibration Slope:", calibration_slope, "\n")
cat("Calibration Intercept:", calibration_intercept, "\n")
cal <- calibrate(final_model, method = "boot", B = 1000)
plot(cal, main = "Calibration Curve")
val.prob.ci.2(cal$prob, cal$ci, digits = 3)
cal
val.prob.ci.2(di$predicted_risk, di$chd69)
CalibrationCurves::val.prob.ci.2(di$predicted_risk, di$chd69)
library(CalibrationCurves)
install.packages("xfun")
install.packages("xfun")
library(CalibrationCurves)
install.packages("xfun")
install.packages("xfun")
#| autorun: true
#| results: 'asis'
library(knitr) # kable
library(survival) # Surv, survfit
library(biostat3) # colon_sample, lifetab2
knitr::kable(biostat3::colon_sample, "html")
colon_cancer_table <- knitr::kable(biostat3::colon_sample, "html")
biostat3::colon_sample
colon_cancer_table <- biostat3::colon_sample
View(colon_cancer_table)
library(biostat3)
melanoma = biostat3::melanoma |>
subset(stage=="Localised") |>
transform(death_cancer = ifelse( status == "Dead: cancer", 1, 0),
death_all = ifelse( status == "Dead: cancer" |
status == "Dead: other", 1, 0))
View(melanoma)
library(knitr)
head(melanoma) |> knitr::kable("html")
head(melanoma)
library(survival)
View(melanoma)
write.csv(melanoma, "melanoma_localised.csv", row.names = FALSE)
pwd
setwd("~/Documents/GitHub/5BD001")
library(collett)
melanoma$calendar_period <- ifelse(melanoma$year8594 == "Diagnosed 75-84", "1975–1984", "1985–1994")
surv_object <- Surv(time = melanoma$surv_mm, event = melanoma$death_cancer)
km_fit <- survfit(surv_object ~ calendar_period, data = melanoma)
View(km_fit)
plot(km_fit,
col = c("blue", "red"),
lwd = 2,
conf.int = TRUE,
main = "Kaplan-Meier Survival Curves by Calendar Period",
xlab = "Time in months",
ylab = "Survival Probability")
legend("topright", legend = levels(melanoma$calendar_period),
col = c("blue", "red"), lwd = 2)
